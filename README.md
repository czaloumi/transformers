**********************************************
# Transformers
**********************************************

#### Author: Chelsea Zaloumis
*Last update: 4/15/2021*

 <p align="center">
 <img src="https://github.com/czaloumi/transformers/blob/main/images/transformers-movie.jpeg" width="75%" height="75%"/>
 </p>

A lecture-style exploration of transformers following Jay Alammar's post [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/). Includes breakout questions and motivating examples.

## Lecture objectives:
1. Motivation for Transformers
2. Define Transformers
3. Define Self-Attention
    1. Self-Attention with vectors
    2. Self-Attention with matrices
4. Define Multi-Head Attention
5. Define Encoder-Decoder Attention layer
6. Final Linear & Softmax Layers
7. Loss Function

## References/Resources
* [Visualizing Machine Neural Translation](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Illustrated Transformers](http://jalammar.github.io/illustrated-transformer/)

* [Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
    For word embedding weights
* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
* [Layer Normalization](https://arxiv.org/abs/1607.06450)
* [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)

## Further Work
1. Coding a basic transformer for natural language processing.
2. Coding a not-so-basic transformer for tbd application.
