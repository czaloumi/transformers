**********************************************
# Transformers
**********************************************

#### Author: Chelsea Zaloumis
*Last update: 4/15/2021*

 <p align="center">
 <img src="https://github.com/czaloumi/transformers/blob/master/images/transformers.png" width="75%" height="75%"/>
 </p>

A lecture-style exploration of transformers following Jay Alammar's post [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/). Includes breakout questions and motivating examples.

## Lecture objectives:
1. Motivation for Transformers
2. Define Transformers
3. Define Self-Attention
    1. Self-Attention with vectors
    2. Self-Attention with matrices
4. Define Multi-Head Attention
5. Define Encoder-Decoder Attention layer
6. Final Linear & Softmax Layers
7. Loss Function

To be added:
1. Coding a basic transformer for natural language processing.
2. Coding a not-so-basic transformer for tbd application.
