{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Transformers</center>\n",
    "<center><img src=\"images/transformers-movie.jpeg\"  width=\"75%\" height=\"75%\"/></center>\n",
    "\n",
    "#### <center>Chelsea Zaloumis</center>\n",
    "<center>4/16/21</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Disclaimer: It is not my intent to claim any material as my own. The information provided in these slides has been collected and interpreted from various online resources as well as published papers. At the end of the slides is a list of references for all that was used to help build these slides. Quotes and proper citations exist throughout the slides to give credit where it's due.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Objectives\n",
    "---\n",
    "1. Motivation for Transformers\n",
    "    1. What is Seq2Seq modeling?\n",
    "    2. Previous methods and their set backs\n",
    "2. Define Transformers\n",
    "3. Calculate Self-Attention\n",
    "    1. Self-Attention with vectors\n",
    "    2. Self-Attention with matrices\n",
    "4. Calculate Multi-Head Attention\n",
    "5. The Encoder-Decoder Attention layer\n",
    "6. Final Linear & Softmax layers\n",
    "7. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "---\n",
    "Just what do we use Transformers for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Transformer is a type of neural network architecture developed to solve the problem of **sequence transduction** or **neural machine translation**: any task that transforms an input sequence to an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* speech recognition\n",
    "* text-to-speech transformation\n",
    "* language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "Useful when words from a sentence refer to phrases prior:\n",
    "\n",
    "\"Tomato basil is my favorite starter because it is surprisingly filling.\" \n",
    "\n",
    "*it* referring to *tomato basil*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "Other Use Cases:\n",
    "* [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.biorxiv.org/content/10.1101/622803v2)\n",
    "* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)\n",
    "* [Selfie: Self-supervised Pretraining for Image Embedding](https://arxiv.org/abs/1906.02940)\n",
    "* [Behavior Sequence Transformer for E-commerce\n",
    "Recommendation in Alibaba](https://arxiv.org/pdf/1905.06874.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "Ok but what's Seq2Seq?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequence-2-Sequence models take a sequence as input and output another sequence. They are comprised of two components: an **Encoder** and a **Decoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/seq2seq.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "\n",
    "<center><img src=\"images/seq2seq.png\"/></center>\n",
    "\n",
    "\"The **Encoder** turns each item into a corresponding hidden vector containing the *item and its context*. The **Decoder** reverses the process, turning the vector into an output item, using the previous output as the input context.\" --[seq2seq model in Machine Learning](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/)\n",
    "\n",
    "This *context* of each input of the sequence in question is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "A Seq2Seq model we are already familiar with is a **Recurrent Neural Network (RNN)**.\n",
    "\n",
    "<img src=\"images/unrolled_rnn.png\" width=\"75%\" height=\"75%\"/>\n",
    "\n",
    "RNNs become inefficient when there is a large gap between relevant info. The longer the chain of info, the more likely important info will get lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "RNNs update its hidden state based on its previous inputs. The hidden state from the final output of the encoder makes it challenging to convey all of the input sequence's *context*. The final hidden state or *context* vector turns out to be a bottleneck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/rnn_decode.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "**Long-Short Term Memory** (LSTM), a type of RNN, tried to solve this problem of learning long-term dependencies by remembering relevant info and forgetting less important info. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of keeping the context from one piece of info that is far from another piece of info decreases exponentially with the distance between inputs of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "And lastly, BOTH RNNs and LSTMs are hard to parallelize. They must go *step-by-step*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**sequential = SLOWWWWWW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "To sum up the problems of RNNs & LSTMs in 3 points:\n",
    "1. Sequential computation inhibits parallelization\n",
    "2. No explicit modeling of long and short range dependencies\n",
    "3. \"Distance\" between positions is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "**Attention** was first introduced by [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473) & [Luong et al., 2015](https://arxiv.org/abs/1508.04025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A technique for paying *attention* to specifics of the information provided; like listening more carefully to the segment I'm writing down during an audio recording or paying attention to certain objects in a room while describing your surroundings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "**Attention** models differ from the previous Seq2Seq models in two ways:\n",
    "1. Each word from a sentence has a corresponding encoded hidden state that is passed all the way to the decoding stage.\n",
    "\n",
    "<img src=\"images/attention.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "2. The decoder looks at each encoded hidden state it received, gives each a score, then multiplies each hidden state by its *softmax score* to amplify hidden states with higher scores.\n",
    "\n",
    "<img src=\"images/attention-gif.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "**Attention** is essentially a method for amplifying signal from the relevant parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Models long and short range dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "We're not out of the woods yet. There are still two problems with RNNs and LSTMs:\n",
    "1. Sequential computation inhibits parallelization\n",
    "2. \"Distance\" between positions is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "* Each input can be processed/encoded at the same time\n",
    "* Distance between positions is logarithmic of $N$ where $N$ is the size of the height of the tree generated from the output to the input below (however many hidden layers)\n",
    "\n",
    "<img src=\"images/wavenet.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation\n",
    "---\n",
    "CNNs combined with Attention allow us to parallelize computation with logarithmic distances b/t inputs, while also figuring out the problem of dependencies when translating sentences! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And yes, there's a name for CNN-Attention combos: **TRANSFORMERS!!!** First introduced in [Attention is All You Need](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Breakout\n",
    "---\n",
    "1. List several previous methods for dealing with sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs (Recurrent Neural Network) and LSTMs (Long-Short Term Memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. What are the three problems with these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(1) There is no modeling of dependencies amongst pieces of sequential data or these methods cannot model dependencies *well*, (2) distance between inputs of a sequence are in linear computation time, (3) computations cannot be parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Breakout\n",
    "---\n",
    "3. How does **Attention** solve one of the three problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Attention models dependencies amongst pieces of sequential data by amplifying signal around more important inputs of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. How do **CNNs** solve the last of the three problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs allow for parallelized computation and reduce computation to logarithmic time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
